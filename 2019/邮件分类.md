## 贝叶斯分类

$$
p(c_i|x,y) = \frac{p(x,y|c_i)}{p(c_i)}
$$

基本的思想是贝叶斯定理，即使用先验概率和条件概率估计后验概率

举例来说,数据有两类，为A或B

每个数据有两个特征，X特征取值为1或2，Y特征取值a或b

| 类别  | A    | A    | B    | B    | A    |
| ----- | ---- | ---- | ---- | ---- | ---- |
| X特征 | 1    | 2    | 2    | 1    | 1    |
| Y特征 | a    | b    | a    | a    | b    |



首先计算先验概率，$p(A)=\frac{3}{5},p(B)=\frac{2}{5}$

先验概率即为该类在总数据中占的比例

之后计算条件概率
$$
p(X=1|A)=\frac{2}{3}，\quad p(X=1|B)=\frac{1}{2}\\
p(X=2|A)=\frac{1}{3}, \quad p(X=2|B)=\frac{1}{2}\\
p(Y=a|A)=\frac{1}{3}, \quad p(Y=a|B)=1\\
P(Y=b|A)=\frac{2}{3}, \quad p(Y=b|B)=0\\
$$
即为在确定是A类的情况下，X=1的概率、X=2的概率等

B类的条件概率同理



最后是计算后验概率

例如我们现在得知一个不确定类别的样本（2，b）

对其进行计算
$$
P(A)*P(X=2|A)*P(Y=b|A)=\frac{2}{15}\\
P(B)*P(X=2|B)*P(Y=b|B)=0\\
$$
因此是A类的概率比B类大

但是这个例子也比较极端，不过大致思想如此



## 编程

首先需要有样本，这里样本分为训练集和测试集

训练集为垃圾邮件和正常邮件各5000封

使用jieba分词对训练集的邮件进行处理，建立一个词典



之后对每一封邮件进行处理，用一个向量进行标识

某一个词出现则为1，没有出现标记为0

每一个词相当于一个特征，用上面同样的原理写出分类器

首先生成词典部分

![1.png][1]

这个函数的返回值是一个列表，是训练集中出现的所有词构成的一个词典

之后是训练分类器的部分

![2.png][2]

主要功能就是分别计算垃圾邮件和普通邮件的先验概率

每一封邮件用一个向量标识，最终对这些向量纵向求和

得到的向量长度为词典长度，每一个值范围在0-5000之间

(这是因为每一类总的邮件数为5000，若每一封垃圾邮件都有这个词，值为5000)



最后验证分类器的效果

![3.png][3]

这部分代码是使用了垃圾邮件的测试集进行验证

如果分类正确会输出Right，否则输出Wrong

最后输出错误率

![4.png][4]

使用30封垃圾邮件进行验证，得到的结果中错误率为20%

效果比较一般

## 分析

朴素贝叶斯分类的假设是所有特征之间独立分布

但是显然在邮件中无法保证每个词的分布都是不同的

而当这个词典的大小达到一定程度时错误率会更高

因此对于5000封邮件训练得到的结果，词典大小为35700个词

显然这些词的分布不是独立的，因此效果不是很好

当样本集比较小，特征数量比较少时贝叶斯分类会有更好的效果


[1]: http://42.193.111.59/usr/uploads/2021/01/895864571.png#vwid=579&vhei=788
[2]: http://42.193.111.59/usr/uploads/2021/01/2382240291.png#vwid=587&vhei=735
[3]: http://42.193.111.59/usr/uploads/2021/01/3338193989.png#vwid=600&vhei=905
[4]: http://42.193.111.59/usr/uploads/2021/01/3341735967.png#vwid=536&vhei=779